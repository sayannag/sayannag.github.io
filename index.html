<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sayan Nag</title>
  
  <meta name="author" content="Sayan Nag">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>
  
  <style>
body {
  background-color: white;
  font-family: cursive;
}

.glow {
  font-size: 15px;
  color: red;
  text-align: center;
  animation: glow 1s ease-in-out infinite alternate;
}

@-webkit-keyframes glow {
  from {
    text-shadow: 0 0 2px #fff, 0 0 3px #fff, 0 0 4px #e60073, 0 0 5px #e60073, 0 0 6px #e60073, 0 0 7px #e60073, 0 0 8px #e60073;
  }
  
  to {
    text-shadow: 0 0 4px #fff, 0 0 5px #ff4da6, 0 0 6px #ff4da6, 0 0 7px #ff4da6, 0 0 8px #ff4da6, 0 0 9px #ff4da6, 0 0 10px #ff4da6;
  }
}
</style>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/icon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sayan Nag (সায়ন নাগ)</name>
              </p>
              <p>I am currently a Research Scientist at <a href="https://research.adobe.com/">Adobe Research</a>. I have completed my PhD at the <a href="https://www.utoronto.ca/">University of Toronto</a> and my undergraduate studies in Electrical Engineering from <a href="http://www.jaduniv.edu.in/">Jadavpur University</a>, India.
              </p>
              <p>
                <font color="red">We are actively looking for research interns. If you are interested in doing research project(s) at Adobe or collaboration with me, please reach out with your CV.</font>
              </p>
              <!--<p>
                Apart from research, I co-founded <a href="https://www.linkedin.com/company/drivefile">Drivefile</a> which is an accountability system for police brutality. I am the co-creator of a graduate school med-science blog called <a href="https://gradschooldigest.github.io/gradschooldigest/">GradSchoolDigest</a>.
			  </p>-->
              <p style="text-align:center">
                <a href="mailto:snag@adobe.com">Email</a> &nbsp | &nbsp
                <!--<a href="data/SayanNag-CV.pdf">CV</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=K8w4dj4AAAAJ&hl=en">Google Scholar</a> &nbsp | &nbsp
                <a href="https://twitter.com/nagsayan112358?lang=en">Twitter</a> &nbsp | &nbsp
                <a href="https://github.com/sayannag">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/Snag1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Snag1circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
			  <div style="width:100%;overflow-y:scroll; height:200px;">
              <ul>
		<li> Slider-Edit and Object-WIPER are going to CVPR (Main), PRISM is going to CVPR (Findings)! <strong><span class="glow">NEW</span></strong></li>
		<li> Agentic-DRS is going to AAAI 2026! <strong><span class="glow">NEW</span></strong></li>
        <li> Localizing Knowledge in DiTs and MAGNET are accepted at NeurIPS 2025!</li>
        <li> AVTrustBench, Aurelia and EgoAdapt are going to ICCV 2025!</li>
        <li> Recognized as an <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee">outstanding reviewer</a> at CVPR 2025! </li>
			  <li> SafaRi and Meerkat are accepted at ECCV 2024!</li>
			  <li> EgoVLPv2 is awarded as an EgoVis (Egocentric Vision) 2022/2023 Distinguished Paper (<a href="https://egovis.github.io/awards/2022_2023/">news</a>)! </li>
			  <li> VistaLLM and MelFusion are selected as <span style='color:red'><strong>Highlights</strong></span> (<span style='color:red'><strong>Top 2.8%</strong></span> of submitted papers) at CVPR 2024! </li>
			  <li> VistaLLM and MelFusion are accepted at CVPR 2024! </li>
			  <li> ApoLLo has been accepted at EMNLP 2023!</li>
			  <li> VoLTA has been accepted at TMLR 2023!</li>
			  <li> EgoVLPv2 has been accepted at ICCV 2023!</li>
			  <li> Joined <a href="https://research.adobe.com/"><span style='color:red'><strong>Adobe Research</strong></span></a> as a research intern!</li>
			  <li> BeAts has been accepted at Interspeech 2023 as an <strong>Oral presentation</strong>!</li>
			  <li> DeCAtt has been accepted at CVPRw 2023 as an <strong>Oral presentation</strong>!</li>
			  <li> IDEAL has been accepted at ICASSP 2023!</li>
		      <li> Our paper on SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function has been accepted at WACV 2023!</li>
              <li> Our paper on Deciphering Environmental Air Pollution with Large Scale City Data has been accepted at at IJCAI 2022 as an <strong>Oral Presentation</strong>!</li>
			  <li> Our abstract on Fast and scalable estimation of effective connectivity using Neural Network aided P-DCM has been accepted at the OHBM, 2022!</li>
			  <li> My paper on Graph Self Supervised Learning: the BT, the HSIC, and the VICReg has been presented at IJCAI Weakly Supervised Representation Learning Workshop 2021!</li>
			  <li> Our paper on CDF-Net: Cross-Domain Fusion Network for Accelerated MRI Reconstruction has been presented at MICCAI 2020!</li>
			</ul>
			</div>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests broadly include <b>Computer Vision</b>, <b>Self-supervised Learning</b>, <b>Multimodal Learning</b>, <b>Time-series Modeling</b> and <b>Natural Language Understanding</b>. Previously, I also worked in ML for Climate Change <i class="fa fa-globe" style="color:blue"></i>, ML for Health<i class='	fa fa-heartbeat' style='color:red'></i>, Approximate Optimzation Algorithms and Image Processing. Some <strong>recent</strong> representative papers can be found below. Other publications can be found in my <a href="https://scholar.google.com/citations?user=K8w4dj4AAAAJ&hl=en">Google Scholar</a> link.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/slider_edit.gif" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control</papertitle>
              <br>
              Arman Zarei, Samyadeep Basu, Mobina Pournemat, <strong>Sayan Nag</strong>, Ryan Rossi, Soheil Feizi
              <br>
              <font color="brown"><em>CVPR</em>, 2026</font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2511.09715">Paper</a> | <a href="https://armanzarei.github.io/SliderEdit/">Project</a> | <a href="https://github.com/ArmanZarei/SliderEdit">Code</a>
              <p></p>
            </td>
          </tr>
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/object_wiper.gif" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Object-WIPER: Training-Free Object and Associated Effect Removal in Videos</papertitle>
              <br>
              Saksham Singh Kushwaha, <strong>Sayan Nag</strong>, Yapeng Tian, Kuldeep Kulkarni
              <br>
              <font color="brown"><em>CVPR</em>, 2026</font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2601.06391">Paper</a> | <a href="https://sakshamsingh1.github.io/object_wiper_webpage/">Project</a>		
              <p></p>
            </td>
          </tr>
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/prism.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Through the PRISM: Principle-Aware, Interpretable, and Multi-Scale Evaluation of Visual Designs</papertitle>
              <br>
              Mona Gandhi, Joseph K J, srinivasan parthasarathy, <strong>Sayan Nag</strong>
              <br>
              <font color="brown"><em>CVPR Findings</em>, 2026</font>
              <br>
			  <p> <a href="">Paper</a> | <a href="">Project</a>		
              <p></p>
            </td>
          </tr>
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/adrs.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Agentic Design Review System</papertitle>
              <br>
              <strong>Sayan Nag</strong>, K J Joseph, Koustava Goswami, Vlad I Morariu, Balaji Vasan Srinivasan
              <br>
              <font color="brown"><em>AAAI</em>, 2026</font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2508.10745">Paper</a> | <a href="">Project</a>		
              <p></p>
            </td>
          </tr>
    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/loki.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Localizing Knowledge in Diffusion Transformers</papertitle>
              <br>
              Arman Zarei, Samyadeep Basu, Keivan Rezaei, Zihao Lin, <strong>Sayan Nag</strong>, Soheil Feizi
              <br>
              <font color="brown"><em>NeurIPS</em>, 2025</font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2505.18832">Paper</a> | <a href="https://armanzarei.github.io/Localizing-Knowledge-in-DiTs/">Project</a> | <a href="https://github.com/ArmanZarei/DiT-Knowledge-Localization">Code</a>		
              <p></p>
            </td>
          </tr>

    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/magnet.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks</papertitle>
              <br>
              Sanjoy Chowdhury, Mohamed Elmoghany, Yohan Abeysinghe, Junjie Fei, <strong>Sayan Nag</strong>, Salman Khan, Mohamed Elhoseiny,  Dinesh Manocha
              <br>
              <font color="brown"><em>NeurIPS</em>, 2025</font>
              <br>
			  <p> <a href="https://www.arxiv.org/pdf/2506.07016">Paper</a> | <a href="https://schowdhury671.github.io/magnet_project/">Project</a>				
              <p></p>
            </td>
          </tr>
		
		<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/avtrustbench.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs</papertitle>
              <br>
              Sanjoy Chowdhury*, <strong>Sayan Nag*</strong>, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha
              <br>
              <font color="brown"><em>ICCV</em>, 2025</font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2501.02135">Paper</a> | <a href="https://schowdhury671.github.io/avtrustbench_project/">Project</a> | <a href="https://github.com/schowdhury671/avtrustbench-">Code</a>	
              <p></p>
            </td>
          </tr>

    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/aurelia.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs</papertitle>
              <br>
              Sanjoy Chowdhury*, Hanan Gani*, Nishit Anand, <strong>Sayan Nag</strong>, Ruohan Gao, Mohamed Elhoseiny, Salman Khan, Dinesh Manocha
              <br>
              <font color="brown"><em>ICCV</em>, 2025</font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2503.23219">Paper</a> | <a href="https://schowdhury671.github.io/aurelia_project/">Project</a> | <a href="https://github.com/schowdhury671/aurelia">Code</a>	
              <p></p>
            </td>
          </tr>

    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/egoadapt.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception</papertitle>
              <br>
              Sanjoy Chowdhury, Subrata Biswas, <strong>Sayan Nag</strong>, Tushar Nagarajan, Calvin Murdock, Ishwarya Ananthabhotla, Yijun Qian, Vamsi Krishna Ithapu, Dinesh Manocha, Ruohan Gao
              <br>
              <font color="brown"><em>ICCV</em>, 2025</font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2506.21080">Paper</a> | <a href="https://schowdhury671.github.io/egoadapt_project/">Project</a> | <a href="https://github.com/schowdhury671/EgoAdapt">Code</a>		
              <p></p>
            </td>
          </tr>
		
		<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/safari_overview.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SafaRi: Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation</papertitle>
              <br>
              <strong>Sayan Nag</strong>, Koustava Goswami, Srikrishna Karanam
              <br>
              <font color="brown"><em>ECCV</em>, 2024</font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2407.02389v1">Paper</a> | <a href="https://sayannag.github.io/safari_eccv2024/">Project</a>				
              <p></p>
            </td>
          </tr>
		
		<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/meerkat_overview.png" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time</papertitle>
              <br>
              Sanjoy Chowdhury*, <strong>Sayan Nag*</strong>, Subhrajyoti Dasgupta*, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha
              <br>
              <font color="brown"><em>ECCV</em>, 2024</font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2407.01851">Paper</a> | <a href="https://schowdhury671.github.io/meerkat_project/">Project</a> | <a href="https://github.com/schowdhury671/meerkat">Code</a>				
              <p></p>
            </td>
          </tr>
			
		
		<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/melfusion-diagram.jpg" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models</papertitle>
              <br>
              Sanjoy Chowdhury*, <strong>Sayan Nag*</strong>, Joseph KJ, Balaji Vasan Srinivasan, Dinesh Manocha
              <br>
              <font color="brown"><em>CVPR</em>, 2024</font> &nbsp <font color="red"><strong>(Highlight, Top 2.8%)</strong></font>
              <br>
			  <p> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chowdhury_MeLFusion_Synthesizing_Music_from_Image_and_Language_Cues_using_Diffusion_CVPR_2024_paper.pdf">Paper</a> | <a href="https://schowdhury671.github.io/melfusion_cvpr2024/">Project</a> | <a href="https://github.com/schowdhury671/melfusion/"> Code </a> | <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FMeLFusion%20datasets&ga=1"> Dataset </a>		
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/VistaLLM_Sampling.png" alt="VistaLLM_Sampling" width="150" height="70">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model</papertitle>
              <br>
              Shraman Pramanick*, Guangxing Han*, Rui Hou, <strong>Sayan Nag</strong>, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi
              <br>
              <font color="brown"><em>CVPR</em>, 2024</font> &nbsp <font color="red"><strong>(Highlight, Top 2.8%)</strong></font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2312.12423">Paper</a> | <a href="https://shramanpramanick.github.io/VistaLLM/">Project</a> | Code (Coming Soon)				
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/apollo.jpg" alt="VistaLLM_Sampling" width="150" height="70">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</papertitle>
              <br>
              Sanjoy Chowdhury*, <strong>Sayan Nag*</strong>, Dinesh Manocha
              <br>
              <font color="brown"><em>EMNLP</em>, 2023</font>
              <br>
			  <p> <a href="https://aclanthology.org/2023.emnlp-main.629.pdf">Paper</a> | <a href="https://github.com/schowdhury671/APoLLo">Code</a>	| <a href="https://gamma.umd.edu/pro/vision_language/apollo/">Project</a> 			
              <p></p>
            </td>
          </tr>
		 
		  <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <divI style="text-align: center">
              <img src="images/radar_egovlpv2-transparent.png" alt="EgoVLPv2" width="180" height="165">
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</papertitle>         
            <br>
            Shraman Pramanick, Yale Song, <strong>Sayan Nag</strong>, Kevin Qinghong Lin, Hardik Shah, Mike Z. Shou, Rama Chellappa, Pengchuan Zhang
            <br>
            <font color="brown"><em>ICCV</em>, 2023</font>

        <p> <a href="https://arxiv.org/pdf/2307.05463.pdf">Paper</a> | <a href="https://github.com/facebookresearch/EgoVLPv2">Code</a> | <a href="https://shramanpramanick.github.io/EgoVLPv2/">Project</a>
            </td>
          </tr>
		  
		  <tr>
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div style="text-align: center">
              <img src="images/VoLTA_Alignment.png" alt="VoLTA" width="150" height="115">
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment</papertitle>           
            <br>
            Shraman Pramanick*, Li Jing*, <strong>Sayan Nag*</strong>, Jiachen Zhu, Hardik Shah, Yann LeCun, Rama Chellappa
            <br>
            <font color="brown"><em>TMLR</em>, 2023</font>

        <p> <a href="https://arxiv.org/pdf/2210.04135.pdf">Paper</a> | <a href="https://github.com/ShramanPramanick/VoLTA">Code</a> | <a href="https://shramanpramanick.github.io/VoLTA/">Project</a> </p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/interspeech23.png" alt="VistaLLM_Sampling" width="130" height="100">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion</papertitle>
              <br>
              Ahana Deb*, <strong>Sayan Nag*</strong>, Ayan Mahapatra*, Soumitri Chattopadhyay*, Aritra Marik*, Pijush Kanti Gayen, Shankha Sanyal, Archi Banerjee, Samir Karmakar
              <br>
              <font color="brown"><em>Interspeech</em>, 2023</font> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2306.02680.pdf">Paper</a> | <a href="https://soumitri2001.github.io/BeAts">Project</a> 			
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/icassp23.png" alt="VistaLLM_Sampling" width="150" height="100">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>IDEAL: Improved DEnse locAL Contrastive Learning for Semi-Supervised Medical Image Segmentation</papertitle>
              <br>
              Hritam Basak, Soumitri Chattopadhyay*, Rohit Kundu*, <strong>Sayan Nag*</strong>, Rammohan Mallipeddi
              <br>
              <font color="brown"><em>ICASSP</em>, 2023</font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2210.15075.pdf">Paper</a> | <a href="https://github.com/Rohit-Kundu/IDEAL-ICASSP23">Code</a> | <a href="https://rohit-kundu.github.io/IDEAL-ICASSP23/">Project</a> 			
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvprw23.jpg" alt="VistaLLM_Sampling" width="150" height="70">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>DeCAtt: Efficient Vision Transformers with Decorrelated Attention Heads</papertitle>
              <br>
              Mayukh Bhattacharyya*, Soumitri Chattopadhyay*, <strong>Sayan Nag*</strong>
              <br>
              <font color="brown"><em>CVPRW</em>, 2023</font> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <p><a href="https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bhattacharyya_DeCAtt_Efficient_Vision_Transformers_With_Decorrelated_Attention_Heads_CVPRW_2023_paper.pdf">Paper</a>			
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/icip23.png" alt="VistaLLM_Sampling" width="150" height="120">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Exploring Self-Supervised Representation Learning For Low-Resource Medical Image Analysis</papertitle>
              <br>
              Soumitri Chattopadhyay, Soham Ganguly*, Sreejit Chaudhury*, <strong>Sayan Nag*</strong>, Samiran Chattopadhyay
              <br>
              <font color="brown"><em>ICIP</em>, 2023</font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2303.02245.pdf">Paper</a> | <a href="https://github.com/soumitri2001/SmallDataSSL">Code</a>			
              <p></p>
            </td>
          </tr>
					
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/Airpoll3.png' height="130", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deciphering Environmental Air Pollution with Large Scale City Data</papertitle>
              <br>
			  Mayukh Bhattacharyya*,
              <strong>Sayan Nag*</strong>,
			  Udita Ghosh
              <br>
							<font color="brown"><em>IJCAI</em>, 2022</font> &nbsp <font color="red"><strong>(Spotlight & Oral Presentation)</strong></font>
              <br>
			  <p>* denotes equal contribution</p>
				<a href="https://www.ijcai.org/proceedings/2022/0698.pdf">Paper</a> | <a href="https://mayukh18.github.io/DEAP/">Project</a> | <a href="https://github.com/mayukh18/DEAP">Code</a>
              <p></p>
              <p>Air pollutant forecasting using a novel <strong><span style="font-family:Courier">cosSquareFormer</span></strong>.</p>


</p>
            </td>
          </tr> 
		  
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <center><img src='images/Serf.png' height="150", width="150"></center>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function</papertitle>
              <br>
              <strong>Sayan Nag*</strong>,  Mayukh Bhattacharyya*, Anuraag Mukherjee*, Rohit Kundu*
              <br>
              <font color="brown"><em>WACV</em>, 2023</font>
              <br>
			  <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.pdf">Paper</a>				
              <p></p>
            </td>
          </tr>

    <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/GraphVICRegHSIC.png' height="150", width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Graph Self Supervised Learning: the BT, the HSIC, and the VICReg</papertitle>
              <br>
              <strong>Sayan Nag</strong>
              <br>
							<font color="brown"><em>IJCAI Weakly Supervised Representation Learning Workshop (IJCAI-WSRL)</em>, 2021</font>
              <br>
			  <a href="https://arxiv.org/pdf/2105.12247.pdf">Paper</a> | 
			  <a href="data/IJCAI_WSRL_poster_paper_ID_20_Sayan_Nag.pdf">Poster</a>
              <p></p>
              <p>Self-Supervised Learning using Graph Neural Networks and a hybrid <strong>VICRegHSIC</strong> loss function.</p>


</p>
            </td>
          </tr> 
		  
		<tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/CDFNet.png' height="150", width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>CDF-Net: Cross-Domain Fusion Network for Accelerated MRI Reconstruction</papertitle>
              <br>
			  Osvald Nitski, 
              <strong>Sayan Nag</strong>, Chris McIntosh, Bo Wang
              <br>
							<font color="brown"><em>MICCAI</em>, 2020</font>
              <br>
			  <a href="https://link.springer.com/chapter/10.1007/978-3-030-59713-9_41">Paper</a>
              <p></p>


</p>
            </td>
          </tr>
		  
		 <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/HybridSiamese.png' height="150", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Hybrid Style Siamese Network: Incorporating style loss in complementary apparels retrieval</papertitle>
              <br>
			  Mayukh Bhattacharyya,
              <strong>Sayan Nag</strong>,
              <br>
              <font color="brown"><em>CVPR Workshop on Computer Vision for Fashion, Art and Design</em>, 2020</font>
              <br>
			  <a href="https://arxiv.org/pdf/1912.05014.pdf">Paper</a> |
			  <a href="https://github.com/mayukh18/Hybrid-Style-Siamese-Network">Code</a> |
			  <a href="https://youtu.be/AyBzlWVhkRw">Video</a>
              <p></p>

</p>
            </td>
          </tr> 
		  
		 <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/DeepMusic.png' height="150", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music</papertitle>
              <br>
              <strong>Sayan Nag</strong>, Medha Basu, Shankha Sanyal, Archi Banerjee, Dipak Ghosh
              <br>
              <font color="brown"><em>Physica A: Statistical Mechanics and its Applications</em>, 2022</font>
              <br>
			  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0378437122002291">Paper</a>
              <p></p>
              <p>A new dataset comprising of <strong>Indian Classical Music</strong> clips is proposed along with a Neural ODE based architecture for MER and MIR tasks.</p>


</p>
            </td>
          </tr> 

        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Some Fun Projects</heading>
              <p>
			  This section consists of some fun projects that I have undertaken during my leisure times.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/DeOldify.png' height="130", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Colorization of Old Movies using Deep Learning</papertitle>
              <br>
              <strong>Sayan Nag</strong>
			  <br>
			  <a href="https://youtu.be/HhV-WBj2tQQ">video</a>
              <p></p>
              <p>This video is a short clip from the movie <a href="https://www.imdb.com/title/tt0060742/">Nayak</a> which shows the famous money scene.
			  This work has been done by using <a href="https://arxiv.org/pdf/1805.08318.pdf">self-attention GAN (SAGAN)</a> which colors and restores old images and videos.
			  This movie is considered as one of the most iconic films in the history of Bengali Cinema. It is also one of my grandmom's favorites.</p>


</p>
            </td>
          </tr> 
		  
		<tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/StyleTransferPaint.png' height="130", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Neural Style Transfer with My Paintings</papertitle>
              <br>
              <strong>Sayan Nag</strong>
			  <br>
              <p></p>
              <p>In this small fun project, I have used my paintings to do <a href="https://en.wikipedia.org/wiki/Neural_style_transfer">Neural Style Transfer</a>.</p>


</p>
            </td>
          </tr>
		  

        </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                Template borrowed from <a href="https://jonbarron.info/">Jon Barron</a>'s website.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
